{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install \"ray[tune]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open3d.ml.torch as ml3d\n",
    "\n",
    "inp_positions = torch.randn([20,3])\n",
    "inp_features = torch.randn([20,8])\n",
    "out_positions = torch.randn([10,3])\n",
    "\n",
    "conv = ml3d.layers.ContinuousConv(in_channels=8, filters=16, kernel_size=[3,3,3])\n",
    "out_features = conv(inp_features, inp_positions, out_positions, extents=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef objective(config):  # ①\\n    score = config[\"a\"] ** 2 + config[\"b\"]\\n    return {\"score\": score}\\n\\n\\nsearch_space = {  # ②\\n    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\\n    \"b\": tune.choice([1, 2, 3]),\\n}\\n\\ntuner = tune.Tuner(objective, param_space=search_space)  # ③\\n\\nresults = tuner.fit()\\nprint(results.get_best_result(metric=\"score\", mode=\"min\").config)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\"\"\"\n",
    "\n",
    "def objective(config):  # ①\n",
    "    score = config[\"a\"] ** 2 + config[\"b\"]\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "search_space = {  # ②\n",
    "    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n",
    "    \"b\": tune.choice([1, 2, 3]),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(objective, param_space=search_space)  # ③\n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"min\").config)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html#tune-pytorch-lightning-ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "\n",
    "        self.outputs = []\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.outputs.append({\"val_loss\": loss, \"val_accuracy\": accuracy})\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in self.outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in self.ouputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "        self.outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_1_size\": 128,\n",
    "    \"layer_2_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import air, tune\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of RayTune metric reportint == Frequency of Lightning Checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "config = {\n",
    "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule(batch_size=64)\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"tune-ptl-example\", version=\".\")\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=MNISTClassifier, config=config)\n",
    "    .trainer(max_epochs=num_epochs, accelerator=accelerator, logger=logger)\n",
    "    .fit_params(datamodule=dm)\n",
    "    .checkpointing(monitor=\"ptl/val_accuracy\", save_top_k=2, mode=\"max\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Make sure to also define an AIR CheckpointConfig here\n",
    "# to properly save checkpoints in AIR format.\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_module_class': __main__.MNISTClassifier,\n",
       " '_module_init_config': {'config': {'layer_1_size': <ray.tune.search.sample.Categorical at 0x7f87aa07fc40>,\n",
       "   'layer_2_size': <ray.tune.search.sample.Categorical at 0x7f87aa07c310>,\n",
       "   'lr': <ray.tune.search.sample.Float at 0x7f87aa07df00>}},\n",
       " '_trainer_init_config': {'max_epochs': 5,\n",
       "  'accelerator': 'gpu',\n",
       "  'logger': <pytorch_lightning.loggers.tensorboard.TensorBoardLogger at 0x7f87aa07f430>},\n",
       " '_trainer_fit_params': {'datamodule': <__main__.MNISTDataModule at 0x7f87aa07c4c0>},\n",
       " '_strategy_config': {},\n",
       " '_model_checkpoint_config': {'monitor': 'ptl/val_accuracy',\n",
       "  'save_top_k': 2,\n",
       "  'mode': 'max'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "# Define a base LightningTrainer without hyper-parameters for Tuner\n",
    "lightning_trainer = LightningTrainer(\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:31,812\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: {'GPU': 1.0, 'node:192.168.195.135': 1.0, 'CPU': 8.0, 'memory': 14585027790.0, 'object_store_memory': 7292513894.0}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "print(\"cuda:\", torch.cuda.is_available())\n",
    "ray.init(num_gpus=1)\n",
    "print(\"ray:\", ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MNISTClassifier(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:33,597\tINFO tuner_internal.py:490 -- A `RunConfig` was passed to both the `Tuner` and the `LightningTrainer`. The run config passed to the `Tuner` is the one that will be used.\n",
      "/tmp/ipykernel_15020/1903400035.py:4: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = tune.Tuner(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-02 09:43:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:19.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.6/25.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 2.0/8 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 4<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0003_2023-07-02_09-42-33/error.txt  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00001_1_layer_1_size=32,layer_2_size=128,lr=0.0836_2023-07-02_09-42-33/error.txt </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0002_2023-07-02_09-42-33/error.txt </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00003_3_layer_1_size=128,layer_2_size=128,lr=0.0538_2023-07-02_09-42-34/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_1_size</th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_2_size</th><th style=\"text-align: right;\">            ..._config/_module_i\n",
       "nit_config/config/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00004</td><td>RUNNING </td><td>192.168.195.135:20290</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00135028 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00005</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.00465507 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00006</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.000237745</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00007</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.000138127</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00008</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00626562 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00009</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.0301398  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td>ERROR   </td><td>192.168.195.135:18899</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.000255166</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td>ERROR   </td><td>192.168.195.135:19257</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.0836217  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td>ERROR   </td><td>192.168.195.135:19606</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.00021693 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td>ERROR   </td><td>192.168.195.135:19948</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.053764   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=18899)\u001b[0m 2023-07-02 09:42:48,163\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['18958 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 2023-07-02 09:42:49,473\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 1 | layer_1  | Linear             | 25.1 K\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 2 | layer_2  | Linear             | 2.1 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 3 | layer_3  | Linear             | 650   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 27.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 27.9 K    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0.112     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:53,807\tERROR tune_controller.py:873 -- Trial task failed for trial LightningTrainer_01be9_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=18899, ip=192.168.195.135, actor_id=6c8ccfdb228077fa060ef3e501000000, repr=LightningTrainer)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=18958, ip=192.168.195.135, actor_id=935068563337b8d41f3745a501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fbfc7227c10>)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/lightning/lightning_trainer.py\", line 553, in _lightning_train_loop_per_worker\n",
      "    trainer.fit(lightning_module, **trainer_fit_params)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1016, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 122, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 244, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 326, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_15020/2719619848.py\", line 54, in on_validation_epoch_end\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1269, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'MNISTClassifier' object has no attribute 'ouputs'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>date               </th><th>hostname  </th><th>node_ip        </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td>2023-07-02_09-42-46</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">18899</td><td style=\"text-align: right;\"> 1688283766</td><td>01be9_00000</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td>2023-07-02_09-42-57</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19257</td><td style=\"text-align: right;\"> 1688283777</td><td>01be9_00001</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td>2023-07-02_09-43-11</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19606</td><td style=\"text-align: right;\"> 1688283791</td><td>01be9_00002</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td>2023-07-02_09-43-24</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19948</td><td style=\"text-align: right;\"> 1688283804</td><td>01be9_00003</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=19257)\u001b[0m 2023-07-02 09:42:59,920\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['19303 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 2023-07-02 09:43:01,514\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 1 | layer_1  | Linear             | 25.1 K\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 2 | layer_2  | Linear             | 4.2 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 3 | layer_3  | Linear             | 1.3 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 30.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 30.6 K    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0.123     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=19606)\u001b[0m 2023-07-02 09:43:13,178\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['19651 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 2023-07-02 09:43:14,426\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 1 | layer_1  | Linear             | 100 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 2 | layer_2  | Linear             | 8.3 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 3 | layer_3  | Linear             | 650   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 109 K     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 109 K     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0.438     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:43:51,181\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_Inner.train()\u001b[39m (pid=20290, ip=192.168.195.135, actor_id=69378a49250e1d9e0ee9cf9a01000000, repr=LightningTrainer)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20335, ip=192.168.195.135, actor_id=34a732ca4f59bf3f92b0447101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ff598807c40>)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/lightning/lightning_trainer.py\", line 553, in _lightning_train_loop_per_worker\n",
      "    trainer.fit(lightning_module, **trainer_fit_params)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1016, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 122, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 244, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 326, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_15020/2719619848.py\", line 54, in on_validation_epoch_end\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1269, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'MNISTClassifier' object has no attribute 'ouputs'\n",
      "2023-07-02 09:43:53,449\tERROR tune.py:1107 -- Trials did not complete: [LightningTrainer_01be9_00000, LightningTrainer_01be9_00001, LightningTrainer_01be9_00002, LightningTrainer_01be9_00003]\n",
      "2023-07-02 09:43:53,451\tINFO tune.py:1111 -- Total run time: 79.72 seconds (66.53 seconds for the tuning loop).\n",
      "2023-07-02 09:43:53,452\tWARNING tune.py:1126 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/jakob/ray_results/tune_mnist_asha\", trainable=...)\n",
      "2023-07-02 09:43:57,261\tWARNING experiment_analysis.py:910 -- Failed to read the results for 10 trials:\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0003_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00001_1_layer_1_size=32,layer_2_size=128,lr=0.0836_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0002_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00003_3_layer_1_size=128,layer_2_size=128,lr=0.0538_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00004_4_layer_1_size=128,layer_2_size=128,lr=0.0014_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00005_5_layer_1_size=128,layer_2_size=64,lr=0.0047_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00006_6_layer_1_size=64,layer_2_size=256,lr=0.0002_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00007_7_layer_1_size=128,layer_2_size=64,lr=0.0001_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00008_8_layer_1_size=64,layer_2_size=128,lr=0.0063_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00009_9_layer_1_size=64,layer_2_size=256,lr=0.0301_2023-07-02_09-42-34\n",
      "2023-07-02 09:43:57,267\tWARNING experiment_analysis.py:777 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     best_result \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mget_best_result(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mptl/val_accuracy\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     best_result\n\u001b[0;32m---> 22\u001b[0m tune_mnist_asha(num_samples\u001b[39m=\u001b[39;49mnum_samples)\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtune_mnist_asha\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m tuner \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mTuner(\n\u001b[1;32m      5\u001b[0m     lightning_trainer,\n\u001b[1;32m      6\u001b[0m     param_space\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mlightning_config\u001b[39m\u001b[39m\"\u001b[39m: lightning_config},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     ),\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mfit()\n\u001b[0;32m---> 18\u001b[0m best_result \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39;49mget_best_result(metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mptl/val_accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m best_result\n",
      "File \u001b[0;32m~/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/result_grid.py:159\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    148\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo best trial found for the given metric: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_analysis\u001b[39m.\u001b[39mdefault_metric\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis means that no trial has reported this metric\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m     error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    157\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "def tune_mnist_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        lightning_trainer,\n",
    "        param_space={\"lightning_config\": lightning_config},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=\"tune_mnist_asha\",\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
    "    best_result\n",
    "\n",
    "\n",
    "tune_mnist_asha(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_parent_folder_import():\n",
    "    import sys, os\n",
    "    sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "enable_parent_folder_import()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsem/Bachelorarbeit/GNNDensityGradients\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages/ray/thirdparty_files',\n",
       " '/home/jsem/Bachelorarbeit/GNNDensityGradients/notebooks',\n",
       " '/home/jsem/Bachelorarbeit/GNNDensityGradients/notebooks/..',\n",
       " '/home/jsem/miniconda3/envs/CConv/lib/python310.zip',\n",
       " '/home/jsem/miniconda3/envs/CConv/lib/python3.10',\n",
       " '/home/jsem/miniconda3/envs/CConv/lib/python3.10/lib-dynload',\n",
       " '',\n",
       " '/home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages',\n",
       " '/tmp/tmpe435t_3b']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import datasets.density_data_module\n",
    "from datasets.density_data_module import *\n",
    "reload(datasets.density_data_module); from datasets.density_data_module import *\n",
    "\n",
    "import datasets.density_dataset\n",
    "from datasets.density_dataset import *\n",
    "reload(datasets.density_dataset); from datasets.density_dataset import *\n",
    "\n",
    "density_data = DensityDataModule(\n",
    "    target = \"temporal_density_gradient\",\n",
    "    data_dir = 'datasets/data/dpi_dam_break/train',\n",
    "    batch_size = 10,\n",
    "    data_split = (0.7, 0.15, 0.15),\n",
    "    num_workers = 0, # Note that cuda only allows 0 workers.\n",
    "    shuffle = False,\n",
    "    cache = False, # Load dataset into memory\n",
    "    device = 'cuda',\n",
    ")\n",
    "# DO NOT SETUP DATA!\n",
    "# density_data.setup(\"fit\")\n",
    "#train_loader = density_data.train_dataloader()\n",
    "#train_iter = iter(train_loader)\n",
    "#batch = next(train_iter)\n",
    "#sample = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from utils.train_helper import *\n",
    "from models.cconv import CConvModel\n",
    "from datasets.density_data_module import DensityDataModule\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder\n",
    "from ray import air, tune, init\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "import torch.cuda\n",
    "\n",
    "\n",
    "hparams = {\n",
    "    # \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    # \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "\n",
    "    # Dataset\n",
    "    'dataset_dir': 'datasets/data/dpi_dam_break/train',\n",
    "    'data_split': (0.7, 0.15, 0.15),\n",
    "    'batch_size': 10,        # care, this is used in the model and datamodule\n",
    "    'shuffle': True,\n",
    "    'cache': True,            # Preprocess and preload dataset into memory\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data module for stage  fit\n"
     ]
    }
   ],
   "source": [
    "density_data.setup(\"fit\")\n",
    "# density_data.to('cpu')\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "train_loader = density_data.train_dataloader()\n",
    "val_loader = density_data.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All masses are zero. Setting masses to 2 * 0.06544984694978737. (This message is only shown once.)\n",
      "pos cuda:0\n",
      "vel cuda:0\n",
      "m cuda:0\n",
      "viscosity cuda:0\n",
      "box cuda:0\n",
      "box_normals cuda:0\n",
      "density cuda:0\n",
      "temporal_density_gradient cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_loader = density_data.train_dataloader()\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "sample = batch[0]\n",
    "\n",
    "# print devices of sample\n",
    "for key in sample.keys():\n",
    "    if isinstance(sample[key], torch.Tensor):\n",
    "        print(key, sample[key].device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "models.cconv.CConvModel"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CConvModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_module_class': models.cconv.CConvModel,\n",
       " '_module_init_config': {'hparams': {'lr': <ray.tune.search.sample.Float at 0x7fbca4e7ada0>,\n",
       "   'dataset_dir': 'datasets/data/dpi_dam_break/train',\n",
       "   'data_split': (0.7, 0.15, 0.15),\n",
       "   'batch_size': 10,\n",
       "   'shuffle': True,\n",
       "   'cache': True,\n",
       "   'device': 'cuda'}},\n",
       " '_trainer_init_config': {'max_epochs': 5,\n",
       "  'logger': <pytorch_lightning.loggers.tensorboard.TensorBoardLogger at 0x7fbaa0265e70>,\n",
       "  'accelerator': 'gpu',\n",
       "  'enable_progress_bar': False},\n",
       " '_trainer_fit_params': {'train_dataloaders': <torch.utils.data.dataloader.DataLoader at 0x7fbca512a6e0>,\n",
       "  'val_dataloaders': <torch.utils.data.dataloader.DataLoader at 0x7fbca5129de0>},\n",
       " '_strategy_config': {},\n",
       " '_model_checkpoint_config': {'monitor': 'val_loss',\n",
       "  'mode': 'min',\n",
       "  'save_top_k': 3}}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.cconv import CConvModel\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"cconv-hparam-search\", version=\".\")\n",
    "datamodule = density_data\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=CConvModel, hparams=hparams)\n",
    "    .trainer(max_epochs=5, logger=logger, accelerator=\"gpu\", enable_progress_bar=False)\n",
    "    .fit_params(train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    .checkpointing(monitor=\"val_loss\", mode=\"min\", save_top_k=3)\n",
    "    .build()\n",
    ")\n",
    "lightning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_dataloaders': <torch.utils.data.dataloader.DataLoader at 0x7fbca512a6e0>,\n",
       " 'val_dataloaders': <torch.utils.data.dataloader.DataLoader at 0x7fbca5129de0>}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_config['_trainer_fit_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 16:30:18,960\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.11</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.5.1</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.11', ray_version='2.5.1', ray_commit='a03efd9931128d387649dd48b0e4864b43d3bfb4', address_info={'node_ip_address': '172.28.36.36', 'raylet_ip_address': '172.28.36.36', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2023-07-04_16-30-17_410749_18854/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2023-07-04_16-30-17_410749_18854/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2023-07-04_16-30-17_410749_18854', 'metrics_export_port': 49694, 'gcs_address': '172.28.36.36:47964', 'address': '172.28.36.36:47964', 'dashboard_agent_listen_port': 52365, 'node_id': '1e9ad8006aa2782f12a5b93d61a6e87215c7b3d4dc9447caa90e6a8b'})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray, os\n",
    "ray.shutdown()\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "env = {'working_dir': parent_dir}\n",
    "ray.init(num_gpus=1, num_cpus=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(datamodule):\n",
    "    pass\n",
    "\n",
    "\n",
    "tune.with_parameters(test, datamodule=datamodule)\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config = CheckpointConfig(\n",
    "        num_to_keep=3,\n",
    "        checkpoint_score_attribute=\"val_loss\",\n",
    "        checkpoint_score_order=\"min\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "scaling_config = ScalingConfig(num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 2, \"GPU\": 0.5})\n",
    "lightning_trainer = LightningTrainer(scaling_config=scaling_config, run_config=run_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7829130390b433485c8555f5015ec91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<LightningTrainer scaling_config=ScalingConfig(num_workers=1, use_gpu=True, resources_per_worker={'CPU': 2, 'GPU': 0.5}) run_config=RunConfig(checkpoint_config=CheckpointConfig(num_to_keep=3, checkpoint_score_attribute='val_loss', checkpoint_score_order='min'), verbose=3)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18854/2746253487.py:2: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = tune.Tuner(\n"
     ]
    }
   ],
   "source": [
    "scheduler = ASHAScheduler(max_t=3, grace_period=1, reduction_factor=2)\n",
    "tuner = tune.Tuner(\n",
    "    lightning_trainer,\n",
    "    param_space={\"lightning_config\": lightning_config},\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=2,\n",
    "        scheduler=scheduler,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-04 16:57:10</td></tr>\n",
       "<tr><td>Running for: </td><td>00:26:25.20        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.4/12.4 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=2<br>Bracket: Iter 2.000: -0.6081217601895332 | Iter 1.000: -0.6718036532402039<br>Logical resource usage: 3.0/6 CPUs, 0.5/1 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">            ...config/_module_in\n",
       "it_config/hparams/lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_loss_step</th><th style=\"text-align: right;\">  val_loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_5ca9d_00000</td><td>TERMINATED</td><td>172.28.36.36:20722</td><td style=\"text-align: right;\">0.000350365</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1580.44</td><td style=\"text-align: right;\">    0.212312</td><td style=\"text-align: right;\">         0.215088</td><td style=\"text-align: right;\">  0.197284</td></tr>\n",
       "<tr><td>LightningTrainer_5ca9d_00001</td><td>TERMINATED</td><td>172.28.36.36:20723</td><td style=\"text-align: right;\">0.0483082  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         1577.92</td><td style=\"text-align: right;\">    0.971411</td><td style=\"text-align: right;\">         0.985242</td><td style=\"text-align: right;\">  0.966885</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=20722)\u001b[0m 2023-07-04 16:30:50,355\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['20853 (172.28.36.36)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 2023-07-04 16:30:51,062\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m   | Name       | Type          | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m ---------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 0 | param_list | ParameterList | 686 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m ---------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 686 K     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 686 K     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m 2.747     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m /home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(LightningTrainer pid=20723)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m /home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20853)\u001b[0m /home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m /home/jsem/miniconda3/envs/CConv/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:432: PossibleUserWarning: It is recommended to use `self.log('train_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m ---------------------------------------------\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   rank_zero_warn(\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=20858)\u001b[0m   warning_cache.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>_report_on     </th><th>date               </th><th>done  </th><th style=\"text-align: right;\">  epoch</th><th>hostname       </th><th style=\"text-align: right;\">  iterations_since_restore</th><th>node_ip     </th><th style=\"text-align: right;\">  pid</th><th>should_checkpoint  </th><th style=\"text-align: right;\">  step</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  train_loss</th><th style=\"text-align: right;\">  train_loss_epoch</th><th style=\"text-align: right;\">  train_loss_step</th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  val_loss</th><th style=\"text-align: right;\">  val_loss_epoch</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_5ca9d_00000</td><td>train_epoch_end</td><td>2023-07-04_16-57-09</td><td>True  </td><td style=\"text-align: right;\">      2</td><td>DESKTOP-S12DUI1</td><td style=\"text-align: right;\">                         3</td><td>172.28.36.36</td><td style=\"text-align: right;\">20722</td><td>True               </td><td style=\"text-align: right;\">  2052</td><td style=\"text-align: right;\">             1580.44</td><td style=\"text-align: right;\">           519.196</td><td style=\"text-align: right;\">       1580.44</td><td style=\"text-align: right;\"> 1688482629</td><td style=\"text-align: right;\">    0.212312</td><td style=\"text-align: right;\">          0.212312</td><td style=\"text-align: right;\">         0.215088</td><td style=\"text-align: right;\">                   3</td><td>5ca9d_00000</td><td style=\"text-align: right;\">  0.197284</td><td style=\"text-align: right;\">        0.197284</td></tr>\n",
       "<tr><td>LightningTrainer_5ca9d_00001</td><td>train_epoch_end</td><td>2023-07-04_16-57-07</td><td>True  </td><td style=\"text-align: right;\">      2</td><td>DESKTOP-S12DUI1</td><td style=\"text-align: right;\">                         3</td><td>172.28.36.36</td><td style=\"text-align: right;\">20723</td><td>True               </td><td style=\"text-align: right;\">  2052</td><td style=\"text-align: right;\">             1577.92</td><td style=\"text-align: right;\">           518.311</td><td style=\"text-align: right;\">       1577.92</td><td style=\"text-align: right;\"> 1688482627</td><td style=\"text-align: right;\">    0.971411</td><td style=\"text-align: right;\">          0.971411</td><td style=\"text-align: right;\">         0.985242</td><td style=\"text-align: right;\">                   3</td><td>5ca9d_00001</td><td style=\"text-align: right;\">  0.966885</td><td style=\"text-align: right;\">        0.966885</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-04 16:57:10,521\tINFO tune.py:1111 -- Total run time: 1585.26 seconds (1585.15 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResultGrid<[\n",
       "  Result(\n",
       "    metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.212312251329422, 'train_loss_step': 0.21508750319480896, 'val_loss': 0.19728408753871918, 'val_loss_epoch': 0.19728408753871918, 'train_loss_epoch': 0.212312251329422, 'epoch': 2, 'step': 2052, 'should_checkpoint': True, 'done': True, 'trial_id': '5ca9d_00000', 'experiment_tag': '0_lr=0.0004'},\n",
       "    path='/home/jsem/ray_results/LightningTrainer_2023-07-04_16-30-36/LightningTrainer_5ca9d_00000_0_lr=0.0004_2023-07-04_16-30-45',\n",
       "    checkpoint=LightningCheckpoint(local_path=/home/jsem/ray_results/LightningTrainer_2023-07-04_16-30-36/LightningTrainer_5ca9d_00000_0_lr=0.0004_2023-07-04_16-30-45/checkpoint_000002)\n",
       "  ),\n",
       "  Result(\n",
       "    metrics={'_report_on': 'train_epoch_end', 'train_loss': 0.9714113473892212, 'train_loss_step': 0.9852420091629028, 'val_loss': 0.9668849110603333, 'val_loss_epoch': 0.9668849110603333, 'train_loss_epoch': 0.9714113473892212, 'epoch': 2, 'step': 2052, 'should_checkpoint': True, 'done': True, 'trial_id': '5ca9d_00001', 'experiment_tag': '1_lr=0.0483'},\n",
       "    path='/home/jsem/ray_results/LightningTrainer_2023-07-04_16-30-36/LightningTrainer_5ca9d_00001_1_lr=0.0483_2023-07-04_16-30-45',\n",
       "    checkpoint=LightningCheckpoint(local_path=/home/jsem/ray_results/LightningTrainer_2023-07-04_16-30-36/LightningTrainer_5ca9d_00001_1_lr=0.0483_2023-07-04_16-30-45/checkpoint_000002)\n",
       "  )\n",
       "]>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner.fit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d_test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
