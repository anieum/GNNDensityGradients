{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install \"ray[tune]\" --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open3d.ml.torch as ml3d\n",
    "\n",
    "inp_positions = torch.randn([20,3])\n",
    "inp_features = torch.randn([20,8])\n",
    "out_positions = torch.randn([10,3])\n",
    "\n",
    "conv = ml3d.layers.ContinuousConv(in_channels=8, filters=16, kernel_size=[3,3,3])\n",
    "out_features = conv(inp_features, inp_positions, out_positions, extents=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef objective(config):  # ①\\n    score = config[\"a\"] ** 2 + config[\"b\"]\\n    return {\"score\": score}\\n\\n\\nsearch_space = {  # ②\\n    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\\n    \"b\": tune.choice([1, 2, 3]),\\n}\\n\\ntuner = tune.Tuner(objective, param_space=search_space)  # ③\\n\\nresults = tuner.fit()\\nprint(results.get_best_result(metric=\"score\", mode=\"min\").config)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray import tune\n",
    "\"\"\"\n",
    "\n",
    "def objective(config):  # ①\n",
    "    score = config[\"a\"] ** 2 + config[\"b\"]\n",
    "    return {\"score\": score}\n",
    "\n",
    "\n",
    "search_space = {  # ②\n",
    "    \"a\": tune.grid_search([0.001, 0.01, 0.1, 1.0]),\n",
    "    \"b\": tune.choice([1, 2, 3]),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(objective, param_space=search_space)  # ③\n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"min\").config)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/latest/tune/examples/tune-pytorch-lightning.html#tune-pytorch-lightning-ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import torch.nn.functional as F\n",
    "from filelock import FileLock\n",
    "from torchmetrics import Accuracy\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "from ray.train.lightning import LightningTrainer, LightningConfigBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTClassifier(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=10)\n",
    "        self.layer_1_size = config[\"layer_1_size\"]\n",
    "        self.layer_2_size = config[\"layer_2_size\"]\n",
    "        self.lr = config[\"lr\"]\n",
    "\n",
    "        # mnist images are (1, 28, 28) (channels, width, height)\n",
    "        self.layer_1 = torch.nn.Linear(28 * 28, self.layer_1_size)\n",
    "        self.layer_2 = torch.nn.Linear(self.layer_1_size, self.layer_2_size)\n",
    "        self.layer_3 = torch.nn.Linear(self.layer_2_size, 10)\n",
    "\n",
    "        self.outputs = []\n",
    "\n",
    "    def cross_entropy_loss(self, logits, labels):\n",
    "        return F.nll_loss(logits, labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, width, height = x.size()\n",
    "        x = x.view(batch_size, -1)\n",
    "\n",
    "        x = self.layer_1(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        x = self.layer_3(x)\n",
    "        x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "\n",
    "        self.log(\"ptl/train_loss\", loss)\n",
    "        self.log(\"ptl/train_accuracy\", accuracy)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = self.cross_entropy_loss(logits, y)\n",
    "        accuracy = self.accuracy(logits, y)\n",
    "        self.outputs.append({\"val_loss\": loss, \"val_accuracy\": accuracy})\n",
    "        return {\"val_loss\": loss, \"val_accuracy\": accuracy}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        avg_loss = torch.stack([x[\"val_loss\"] for x in self.outputs]).mean()\n",
    "        avg_acc = torch.stack([x[\"val_accuracy\"] for x in self.ouputs]).mean()\n",
    "        self.log(\"ptl/val_loss\", avg_loss)\n",
    "        self.log(\"ptl/val_accuracy\", avg_acc)\n",
    "        self.outputs = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, batch_size=128):\n",
    "        super().__init__()\n",
    "        self.data_dir = os.getcwd()\n",
    "        self.batch_size = batch_size\n",
    "        self.transform = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        with FileLock(f\"{self.data_dir}.lock\"):\n",
    "            mnist = MNIST(\n",
    "                self.data_dir, train=True, download=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist, [55000, 5000])\n",
    "\n",
    "            self.mnist_test = MNIST(\n",
    "                self.data_dir, train=False, download=True, transform=self.transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=self.batch_size, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"layer_1_size\": 128,\n",
    "    \"layer_2_size\": 256,\n",
    "    \"lr\": 1e-3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from ray import air, tune\n",
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.tune.schedulers import ASHAScheduler, PopulationBasedTraining"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency of RayTune metric reportint == Frequency of Lightning Checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum training epochs\n",
    "num_epochs = 5\n",
    "\n",
    "# Number of sampls from parameter space\n",
    "num_samples = 10\n",
    "\n",
    "accelerator = \"gpu\"\n",
    "\n",
    "config = {\n",
    "    \"layer_1_size\": tune.choice([32, 64, 128]),\n",
    "    \"layer_2_size\": tune.choice([64, 128, 256]),\n",
    "    \"lr\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = MNISTDataModule(batch_size=64)\n",
    "logger = TensorBoardLogger(save_dir=os.getcwd(), name=\"tune-ptl-example\", version=\".\")\n",
    "\n",
    "lightning_config = (\n",
    "    LightningConfigBuilder()\n",
    "    .module(cls=MNISTClassifier, config=config)\n",
    "    .trainer(max_epochs=num_epochs, accelerator=accelerator, logger=logger)\n",
    "    .fit_params(datamodule=dm)\n",
    "    .checkpointing(monitor=\"ptl/val_accuracy\", save_top_k=2, mode=\"max\")\n",
    "    .build()\n",
    ")\n",
    "\n",
    "# Make sure to also define an AIR CheckpointConfig here\n",
    "# to properly save checkpoints in AIR format.\n",
    "run_config = RunConfig(\n",
    "    checkpoint_config=CheckpointConfig(\n",
    "        num_to_keep=2,\n",
    "        checkpoint_score_attribute=\"ptl/val_accuracy\",\n",
    "        checkpoint_score_order=\"max\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_module_class': __main__.MNISTClassifier,\n",
       " '_module_init_config': {'config': {'layer_1_size': <ray.tune.search.sample.Categorical at 0x7f87aa07fc40>,\n",
       "   'layer_2_size': <ray.tune.search.sample.Categorical at 0x7f87aa07c310>,\n",
       "   'lr': <ray.tune.search.sample.Float at 0x7f87aa07df00>}},\n",
       " '_trainer_init_config': {'max_epochs': 5,\n",
       "  'accelerator': 'gpu',\n",
       "  'logger': <pytorch_lightning.loggers.tensorboard.TensorBoardLogger at 0x7f87aa07f430>},\n",
       " '_trainer_fit_params': {'datamodule': <__main__.MNISTDataModule at 0x7f87aa07c4c0>},\n",
       " '_strategy_config': {},\n",
       " '_model_checkpoint_config': {'monitor': 'ptl/val_accuracy',\n",
       "  'save_top_k': 2,\n",
       "  'mode': 'max'}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightning_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "scaling_config = ScalingConfig(\n",
    "    num_workers=1, use_gpu=True, resources_per_worker={\"CPU\": 1, \"GPU\": 1}\n",
    ")\n",
    "# Define a base LightningTrainer without hyper-parameters for Tuner\n",
    "lightning_trainer = LightningTrainer(\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:31,812\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: {'GPU': 1.0, 'node:192.168.195.135': 1.0, 'CPU': 8.0, 'memory': 14585027790.0, 'object_store_memory': 7292513894.0}\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "print(\"cuda:\", torch.cuda.is_available())\n",
    "ray.init(num_gpus=1)\n",
    "print(\"ray:\", ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MNISTClassifier(default_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:33,597\tINFO tuner_internal.py:490 -- A `RunConfig` was passed to both the `Tuner` and the `LightningTrainer`. The run config passed to the `Tuner` is the one that will be used.\n",
      "/tmp/ipykernel_15020/1903400035.py:4: UserWarning: Executing `.fit()` may leave less than 20% of CPUs in this cluster for Dataset execution, which can lead to resource contention or hangs. To avoid this, reserve at least 20% of node CPUs for Dataset execution by setting `_max_cpu_fraction_per_node = 0.8` in the Trainer scaling_config. See https://docs.ray.io/en/master/data/dataset-internals.html#datasets-and-tune for more info.\n",
      "  tuner = tune.Tuner(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-07-02 09:43:53</td></tr>\n",
       "<tr><td>Running for: </td><td>00:01:19.63        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.6/25.0 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 4.000: None | Iter 2.000: None | Iter 1.000: None<br>Logical resource usage: 2.0/8 CPUs, 1.0/1 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 4<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0003_2023-07-02_09-42-33/error.txt  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00001_1_layer_1_size=32,layer_2_size=128,lr=0.0836_2023-07-02_09-42-33/error.txt </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0002_2023-07-02_09-42-33/error.txt </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td style=\"text-align: right;\">           1</td><td>/home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00003_3_layer_1_size=128,layer_2_size=128,lr=0.0538_2023-07-02_09-42-34/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>status  </th><th>loc                  </th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_1_size</th><th style=\"text-align: right;\">    ...odule_init_config\n",
       "/config/layer_2_size</th><th style=\"text-align: right;\">            ..._config/_module_i\n",
       "nit_config/config/lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00004</td><td>RUNNING </td><td>192.168.195.135:20290</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00135028 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00005</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.00465507 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00006</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.000237745</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00007</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.000138127</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00008</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.00626562 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00009</td><td>PENDING </td><td>                     </td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">256</td><td style=\"text-align: right;\">0.0301398  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td>ERROR   </td><td>192.168.195.135:18899</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.000255166</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td>ERROR   </td><td>192.168.195.135:19257</td><td style=\"text-align: right;\"> 32</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.0836217  </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td>ERROR   </td><td>192.168.195.135:19606</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\"> 64</td><td style=\"text-align: right;\">0.00021693 </td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td>ERROR   </td><td>192.168.195.135:19948</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">128</td><td style=\"text-align: right;\">0.053764   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=18899)\u001b[0m 2023-07-02 09:42:48,163\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['18958 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 2023-07-02 09:42:49,473\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 1 | layer_1  | Linear             | 25.1 K\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 2 | layer_2  | Linear             | 2.1 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 3 | layer_3  | Linear             | 650   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 27.9 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 27.9 K    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=18958)\u001b[0m 0.112     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:42:53,807\tERROR tune_controller.py:873 -- Trial task failed for trial LightningTrainer_01be9_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py\", line 18, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/_private/worker.py\", line 2540, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_Inner.train()\u001b[39m (pid=18899, ip=192.168.195.135, actor_id=6c8ccfdb228077fa060ef3e501000000, repr=LightningTrainer)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=18958, ip=192.168.195.135, actor_id=935068563337b8d41f3745a501000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7fbfc7227c10>)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/lightning/lightning_trainer.py\", line 553, in _lightning_train_loop_per_worker\n",
      "    trainer.fit(lightning_module, **trainer_fit_params)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1016, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 122, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 244, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 326, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_15020/2719619848.py\", line 54, in on_validation_epoch_end\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1269, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'MNISTClassifier' object has no attribute 'ouputs'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                  </th><th>date               </th><th>hostname  </th><th>node_ip        </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>LightningTrainer_01be9_00000</td><td>2023-07-02_09-42-46</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">18899</td><td style=\"text-align: right;\"> 1688283766</td><td>01be9_00000</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00001</td><td>2023-07-02_09-42-57</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19257</td><td style=\"text-align: right;\"> 1688283777</td><td>01be9_00001</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00002</td><td>2023-07-02_09-43-11</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19606</td><td style=\"text-align: right;\"> 1688283791</td><td>01be9_00002</td></tr>\n",
       "<tr><td>LightningTrainer_01be9_00003</td><td>2023-07-02_09-43-24</td><td>Jakob-PC-N</td><td>192.168.195.135</td><td style=\"text-align: right;\">19948</td><td style=\"text-align: right;\"> 1688283804</td><td>01be9_00003</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=19257)\u001b[0m 2023-07-02 09:42:59,920\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['19303 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 2023-07-02 09:43:01,514\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 1 | layer_1  | Linear             | 25.1 K\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 2 | layer_2  | Linear             | 4.2 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 3 | layer_3  | Linear             | 1.3 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 30.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 30.6 K    Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19303)\u001b[0m 0.123     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(LightningTrainer pid=19606)\u001b[0m 2023-07-02 09:43:13,178\tINFO backend_executor.py:137 -- Starting distributed worker processes: ['19651 (192.168.195.135)']\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 2023-07-02 09:43:14,426\tINFO config.py:86 -- Setting up process group for: env:// [rank=0, world_size=1]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m GPU available: True (cuda), used: True\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m   | Name     | Type               | Params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0 | accuracy | MulticlassAccuracy | 0     \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 1 | layer_1  | Linear             | 100 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 2 | layer_2  | Linear             | 8.3 K \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 3 | layer_3  | Linear             | 650   \n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m ------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 109 K     Trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 109 K     Total params\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=19651)\u001b[0m 0.438     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]\u001b[0m \n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-02 09:43:51,181\tERROR worker.py:408 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): \u001b[36mray::_Inner.train()\u001b[39m (pid=20290, ip=192.168.195.135, actor_id=69378a49250e1d9e0ee9cf9a01000000, repr=LightningTrainer)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 389, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 54, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=20335, ip=192.168.195.135, actor_id=34a732ca4f59bf3f92b0447101000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x7ff598807c40>)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/worker_group.py\", line 32, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/_internal/utils.py\", line 129, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/train/lightning/lightning_trainer.py\", line 553, in _lightning_train_loop_per_worker\n",
      "    trainer.fit(lightning_module, **trainer_fit_params)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 531, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 41, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 91, in launch\n",
      "    return function(*args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 570, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 975, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1016, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py\", line 177, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 122, in run\n",
      "    return self.on_run_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 244, in on_run_end\n",
      "    self._on_evaluation_epoch_end()\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 326, in _on_evaluation_epoch_end\n",
      "    call._call_lightning_module_hook(trainer, hook_name)\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py\", line 140, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_15020/2719619848.py\", line 54, in on_validation_epoch_end\n",
      "  File \"/home/jakob/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1269, in __getattr__\n",
      "    raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
      "AttributeError: 'MNISTClassifier' object has no attribute 'ouputs'\n",
      "2023-07-02 09:43:53,449\tERROR tune.py:1107 -- Trials did not complete: [LightningTrainer_01be9_00000, LightningTrainer_01be9_00001, LightningTrainer_01be9_00002, LightningTrainer_01be9_00003]\n",
      "2023-07-02 09:43:53,451\tINFO tune.py:1111 -- Total run time: 79.72 seconds (66.53 seconds for the tuning loop).\n",
      "2023-07-02 09:43:53,452\tWARNING tune.py:1126 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: Tuner.restore(path=\"/home/jakob/ray_results/tune_mnist_asha\", trainable=...)\n",
      "2023-07-02 09:43:57,261\tWARNING experiment_analysis.py:910 -- Failed to read the results for 10 trials:\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00000_0_layer_1_size=32,layer_2_size=64,lr=0.0003_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00001_1_layer_1_size=32,layer_2_size=128,lr=0.0836_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00002_2_layer_1_size=128,layer_2_size=64,lr=0.0002_2023-07-02_09-42-33\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00003_3_layer_1_size=128,layer_2_size=128,lr=0.0538_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00004_4_layer_1_size=128,layer_2_size=128,lr=0.0014_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00005_5_layer_1_size=128,layer_2_size=64,lr=0.0047_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00006_6_layer_1_size=64,layer_2_size=256,lr=0.0002_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00007_7_layer_1_size=128,layer_2_size=64,lr=0.0001_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00008_8_layer_1_size=64,layer_2_size=128,lr=0.0063_2023-07-02_09-42-34\n",
      "- /home/jakob/ray_results/tune_mnist_asha/LightningTrainer_01be9_00009_9_layer_1_size=64,layer_2_size=256,lr=0.0301_2023-07-02_09-42-34\n",
      "2023-07-02 09:43:57,267\tWARNING experiment_analysis.py:777 -- Could not find best trial. Did you pass the correct `metric` parameter?\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m     best_result \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mget_best_result(metric\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mptl/val_accuracy\u001b[39m\u001b[39m\"\u001b[39m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     best_result\n\u001b[0;32m---> 22\u001b[0m tune_mnist_asha(num_samples\u001b[39m=\u001b[39;49mnum_samples)\n",
      "Cell \u001b[0;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtune_mnist_asha\u001b[0;34m(num_samples)\u001b[0m\n\u001b[1;32m      4\u001b[0m tuner \u001b[39m=\u001b[39m tune\u001b[39m.\u001b[39mTuner(\n\u001b[1;32m      5\u001b[0m     lightning_trainer,\n\u001b[1;32m      6\u001b[0m     param_space\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mlightning_config\u001b[39m\u001b[39m\"\u001b[39m: lightning_config},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     ),\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m results \u001b[39m=\u001b[39m tuner\u001b[39m.\u001b[39mfit()\n\u001b[0;32m---> 18\u001b[0m best_result \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39;49mget_best_result(metric\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mptl/val_accuracy\u001b[39;49m\u001b[39m\"\u001b[39;49m, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmax\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     19\u001b[0m best_result\n",
      "File \u001b[0;32m~/anaconda3/envs/o3d_test2/lib/python3.10/site-packages/ray/tune/result_grid.py:159\u001b[0m, in \u001b[0;36mResultGrid.get_best_result\u001b[0;34m(self, metric, mode, scope, filter_nan_and_inf)\u001b[0m\n\u001b[1;32m    148\u001b[0m     error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m    149\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo best trial found for the given metric: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m \u001b[39m\u001b[39mor\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experiment_analysis\u001b[39m.\u001b[39mdefault_metric\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis means that no trial has reported this metric\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m     )\n\u001b[1;32m    153\u001b[0m     error_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, or all values reported for this metric are NaN. To not ignore NaN \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mvalues, you can set the `filter_nan_and_inf` arg to False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[39mif\u001b[39;00m filter_nan_and_inf\n\u001b[1;32m    157\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(error_msg)\n\u001b[1;32m    161\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_trial_to_result(best_trial)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No best trial found for the given metric: ptl/val_accuracy. This means that no trial has reported this metric, or all values reported for this metric are NaN. To not ignore NaN values, you can set the `filter_nan_and_inf` arg to False."
     ]
    }
   ],
   "source": [
    "def tune_mnist_asha(num_samples=10):\n",
    "    scheduler = ASHAScheduler(max_t=num_epochs, grace_period=1, reduction_factor=2)\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        lightning_trainer,\n",
    "        param_space={\"lightning_config\": lightning_config},\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"ptl/val_accuracy\",\n",
    "            mode=\"max\",\n",
    "            num_samples=num_samples,\n",
    "            scheduler=scheduler,\n",
    "        ),\n",
    "        run_config=air.RunConfig(\n",
    "            name=\"tune_mnist_asha\",\n",
    "        ),\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    best_result = results.get_best_result(metric=\"ptl/val_accuracy\", mode=\"max\")\n",
    "    best_result\n",
    "\n",
    "\n",
    "tune_mnist_asha(num_samples=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enable_parent_folder_import():\n",
    "    import sys, os\n",
    "    sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "\n",
    "enable_parent_folder_import()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up data module for stage  fit\n",
      "Loading dataset into memory and applying transform_once... (Device: cuda)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/131 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: All masses are zero. Setting masses to 2 * 0.06544984694978737. (This message is only shown once.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [00:15<00:00,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading dataset into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import datasets.density_data_module\n",
    "from datasets.density_data_module import *\n",
    "reload(datasets.density_data_module); from datasets.density_data_module import *\n",
    "\n",
    "import datasets.density_dataset\n",
    "from datasets.density_dataset import *\n",
    "reload(datasets.density_dataset); from datasets.density_dataset import *\n",
    "\n",
    "density_data = DensityDataModule(\n",
    "    target = \"temporal_density_gradient\",\n",
    "    data_dir = '../datasets/data/dpi_dam_break/train',\n",
    "    batch_size = 10,\n",
    "    data_split = (0.7, 0.15, 0.15),\n",
    "    num_workers = 0, # Note that cuda only allows 0 workers.\n",
    "    shuffle = False,\n",
    "    cache = True, # Load dataset into memory\n",
    "    device = 'cuda',\n",
    ")\n",
    "density_data.setup(\"fit\")\n",
    "train_loader = density_data.train_dataloader()\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "sample = batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datasets.density_data_module.DensityDataModule at 0x7f87b4008280>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density_data.to('cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos cpu\n",
      "vel cpu\n",
      "m cpu\n",
      "viscosity cpu\n",
      "box cpu\n",
      "box_normals cpu\n",
      "density cpu\n",
      "temporal_density_gradient cpu\n"
     ]
    }
   ],
   "source": [
    "train_loader = density_data.train_dataloader()\n",
    "train_iter = iter(train_loader)\n",
    "batch = next(train_iter)\n",
    "sample = batch[0]\n",
    "\n",
    "# print devices of sample\n",
    "for key in sample.keys():\n",
    "    if isinstance(sample[key], torch.Tensor):\n",
    "        print(key, sample[key].device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "o3d_test2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
